{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7770a6e7",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6857dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "#import config\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def readTxt(file_path):\n",
    "    img_list = []\n",
    "    with open(file_path, 'r') as file_to_read:\n",
    "        while True:\n",
    "            lines = file_to_read.readline()\n",
    "            if not lines:\n",
    "                break\n",
    "            item = lines.strip().split()\n",
    "            img_list.append(item)\n",
    "    file_to_read.close()\n",
    "    return img_list\n",
    "\n",
    "class RoadSequenceDatasetList(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, transforms):\n",
    "\n",
    "        self.img_list = readTxt(file_path)\n",
    "        self.dataset_size = len(self.img_list)\n",
    "        self.transforms = transforms\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path_list = self.img_list[idx]\n",
    "        data = []\n",
    "        for i in range(5):\n",
    "            data.append(torch.unsqueeze(self.transforms(Image.open(img_path_list[i])), dim=0))\n",
    "        data = torch.cat(data, 0)\n",
    "        label = Image.open(img_path_list[5])\n",
    "        label = torch.squeeze(self.transforms(label))\n",
    "        sample = {'data': data, 'label': label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b05aa4",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5084d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_path = 'D:/Kuliah/SEMESTER 8/Robust-Lane-Detection-master/Robust-Lane-Detection-master/LaneDetectionCode/data/train_index.txt'\n",
    "val_path = 'D:/Kuliah/SEMESTER 8/Robust-Lane-Detection-master/Robust-Lane-Detection-master/LaneDetectionCode/data/DatasetList.txt'\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "batch_size = 4;\n",
    "from torch.utils.data import DataLoader\n",
    "train_set = RoadSequenceDatasetList(file_path=train_path,transforms=transform)\n",
    "trainloader = DataLoader(train_set,batch_size=batch_size,shuffle=True)\n",
    "val_set = RoadSequenceDatasetList(file_path=val_path,transforms=transform)\n",
    "val_loader = DataLoader(val_set,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "042d04d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e6a464",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed2cc8",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67386d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffX = x1.size()[2] - x2.size()[2]\n",
    "        diffY = x1.size()[3] - x2.size()[3]\n",
    "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
    "                        diffY // 2, int(diffY / 2)))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: (int, int)\n",
    "            Height and width of input tensor as (height, width).\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.height, self.width = input_size\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, self.height, self.width).cuda(),\n",
    "                torch.zeros(batch_size, self.hidden_dim, self.height, self.width).cuda())\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.height, self.width = input_size\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_size=(self.height, self.width),\n",
    "                                          input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor=input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "\n",
    "\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output = layer_output.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed680b2b",
   "metadata": {},
   "source": [
    "## UNet-ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582332e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_ConvLSTM(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet_ConvLSTM, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256)\n",
    "        self.up2 = up(512, 128)\n",
    "        self.up3 = up(256, 64)\n",
    "        self.up4 = up(128, 64)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "        self.convlstm = ConvLSTM(input_size=(8,16),\n",
    "                                 input_dim=512,\n",
    "                                 hidden_dim=[512, 512],\n",
    "                                 kernel_size=(3,3),\n",
    "                                 num_layers=2,\n",
    "                                 batch_first=False,\n",
    "                                 bias=True,\n",
    "                                 return_all_layers=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unbind(x, dim=1)\n",
    "        data = []\n",
    "        for item in x:\n",
    "            x1 = self.inc(item)\n",
    "            x2 = self.down1(x1)\n",
    "            x3 = self.down2(x2)\n",
    "            x4 = self.down3(x3)\n",
    "            x5 = self.down4(x4)\n",
    "            data.append(x5.unsqueeze(0))\n",
    "        data = torch.cat(data, dim=0)\n",
    "        lstm, _ = self.convlstm(data)\n",
    "        test = lstm[0][ -1,:, :, :, :]\n",
    "        x = self.up1(test, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246daae9",
   "metadata": {},
   "source": [
    "# Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c11762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch\n",
    "#from jcopdl.callback import Callback, set_config\n",
    "from _callback import CallbackSemSeg\n",
    "from _config import set_config\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38fa9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = set_config({\n",
    "    'batch_size':batch_size,\n",
    "    'learning_rate':learning_rate\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc23465",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Kuliah/DeepLearning/Code/SemanticSegmentation/4_SemanticSegmentationTry/Tugas Akhir/3_TA_UNetConvLSTM/Epoch51/weights_best.pth'\n",
    "model = UNet_ConvLSTM(3,2).to(device)\n",
    "model.load_state_dict(torch.load(path))\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor([0.02, 2.02]).to(device))\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "callback = CallbackSemSeg(model,config,outdir='Tugas Akhir/3_TA_UNetConvLSTM/Epoch51',plot_every=3,save_every=1,early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a298094",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Kuliah/DeepLearning/Code/SemanticSegmentation/4_SemanticSegmentationTry/Tugas Akhir/3_TA_UNetConvLSTM/Epoch51/weights_best.pth'\n",
    "model = UNet_ConvLSTM(3,2).to(device)\n",
    "model.load_state_dict(torch.load(path))\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor([0.02, 2.02]).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f035b",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4da616d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class SegmentationMetrics(object):\n",
    "    r\"\"\"Calculate common metrics in semantic segmentation to evalueate model preformance.\n",
    "    Supported metrics: Pixel accuracy, Dice Coeff, precision score and recall score.\n",
    "    \n",
    "    Pixel accuracy measures how many pixels in a image are predicted correctly.\n",
    "    Dice Coeff is a measure function to measure similarity over 2 sets, which is usually used to\n",
    "    calculate the similarity of two samples. Dice equals to f1 score in semantic segmentation tasks.\n",
    "    \n",
    "    It should be noted that Dice Coeff and Intersection over Union are highly related, so you need \n",
    "    NOT calculate these metrics both, the other can be calcultaed directly when knowing one of them.\n",
    "    Precision describes the purity of our positive detections relative to the ground truth. Of all\n",
    "    the objects that we predicted in a given image, precision score describes how many of those objects\n",
    "    actually had a matching ground truth annotation.\n",
    "    Recall describes the completeness of our positive predictions relative to the ground truth. Of\n",
    "    all the objected annotated in our ground truth, recall score describes how many true positive instances\n",
    "    we have captured in semantic segmentation.\n",
    "    Args:\n",
    "        eps: float, a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        average: bool. Default: ``True``\n",
    "            When set to ``True``, average Dice Coeff, precision and recall are\n",
    "            returned. Otherwise Dice Coeff, precision and recall of each class\n",
    "            will be returned as a numpy array.\n",
    "        ignore_background: bool. Default: ``True``\n",
    "            When set to ``True``, the class will not calculate related metrics on\n",
    "            background pixels. When the segmentation of background pixels is not\n",
    "            important, set this value to ``True``.\n",
    "        activation: [None, 'none', 'softmax' (default), 'sigmoid', '0-1']\n",
    "            This parameter determines what kind of activation function that will be\n",
    "            applied on model output.\n",
    "    Input:\n",
    "        y_true: :math:`(N, H, W)`, torch tensor, where we use int value between (0, num_class - 1)\n",
    "        to denote every class, where ``0`` denotes background class.\n",
    "        y_pred: :math:`(N, C, H, W)`, torch tensor.\n",
    "    Examples::\n",
    "        >>> metric_calculator = SegmentationMetrics(average=True, ignore_background=True)\n",
    "        >>> pixel_accuracy, dice, precision, recall = metric_calculator(y_true, y_pred)\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-5, average=True, ignore_background=True, activation='0-1'):\n",
    "        self.eps = eps\n",
    "        self.average = average\n",
    "        self.ignore = ignore_background\n",
    "        self.activation = activation\n",
    "\n",
    "    @staticmethod\n",
    "    def _one_hot(gt, pred, class_num):\n",
    "        # transform sparse mask into one-hot mask\n",
    "        # shape: (B, H, W) -> (B, C, H, W)\n",
    "        input_shape = tuple(gt.shape)  # (N, H, W, ...)\n",
    "        new_shape = (input_shape[0], class_num) + input_shape[1:]\n",
    "        one_hot = torch.zeros(new_shape).to(pred.device, dtype=torch.float)\n",
    "        target = one_hot.scatter_(1, gt.unsqueeze(1).long().data, 1.0)\n",
    "        return target\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_class_data(gt_onehot, pred, class_num):\n",
    "        # perform calculation on a batch\n",
    "        # for precise result in a single image, plz set batch size to 1\n",
    "        matrix = np.zeros((3, class_num))\n",
    "\n",
    "        # calculate tp, fp, fn per class\n",
    "        for i in range(class_num):\n",
    "            # pred shape: (N, H, W)\n",
    "            class_pred = pred[:, i, :, :]\n",
    "            #print(class_pred.size())\n",
    "            # gt shape: (N, H, W), binary array where 0 denotes negative and 1 denotes positive\n",
    "            class_gt = gt_onehot[:, i, :, :]\n",
    "            class_gt = transforms.Grayscale(num_output_channels=1)(class_gt)\n",
    "            #print(class_gt.size())\n",
    "            pred_flat = class_pred.contiguous().view(-1, )  # shape: (N * H * W, )\n",
    "            #print(pred_flat.size())\n",
    "            gt_flat = class_gt.contiguous().view(-1, )  # shape: (N * H * W, )\n",
    "            #print(gt_flat.size())\n",
    "\n",
    "            tp = torch.sum(gt_flat * pred_flat)\n",
    "            fp = torch.sum(pred_flat) - tp\n",
    "            fn = torch.sum(gt_flat) - tp\n",
    "\n",
    "            matrix[:, i] = tp.item(), fp.item(), fn.item()\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _calculate_multi_metrics(self, gt, pred, class_num):\n",
    "        # calculate metrics in multi-class segmentation\n",
    "        matrix = self._get_class_data(gt, pred, class_num)\n",
    "        if self.ignore:\n",
    "            matrix = matrix[:, 1:]\n",
    "\n",
    "        # tp = np.sum(matrix[0, :])\n",
    "        # fp = np.sum(matrix[1, :])\n",
    "        # fn = np.sum(matrix[2, :])\n",
    "\n",
    "        pixel_acc = (np.sum(matrix[0, :]) + self.eps) / (np.sum(matrix[0, :]) + np.sum(matrix[1, :]))\n",
    "        dice = (2 * matrix[0] + self.eps) / (2 * matrix[0] + matrix[1] + matrix[2] + self.eps) #F1 Score\n",
    "        precision = (matrix[0] + self.eps) / (matrix[0] + matrix[1] + self.eps)\n",
    "        recall = (matrix[0] + self.eps) / (matrix[0] + matrix[2] + self.eps)\n",
    "        IoU = (matrix[0] + self.eps) / (matrix[0] + matrix[1] + matrix[2] + self.eps)\n",
    "\n",
    "        if self.average:\n",
    "            dice = np.average(dice)\n",
    "            precision = np.average(precision)\n",
    "            recall = np.average(recall)\n",
    "            IoU = np.average(IoU)\n",
    "\n",
    "        return pixel_acc, dice, precision, recall, IoU\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        class_num = y_pred.size(1)\n",
    "\n",
    "        if self.activation in [None, 'none']:\n",
    "            activation_fn = lambda x: x\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            activation_fn = nn.Sigmoid()\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"softmax\":\n",
    "            activation_fn = nn.Softmax(dim=1)\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"0-1\":\n",
    "            pred_argmax = torch.argmax(y_pred, dim=1)\n",
    "            activated_pred = self._one_hot(pred_argmax, y_pred, class_num)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not a supported activation!\")\n",
    "\n",
    "        gt_onehot = self._one_hot(y_true, y_pred, class_num)\n",
    "        pixel_acc, dice, precision, recall, IoU = self._calculate_multi_metrics(gt_onehot, activated_pred, class_num)\n",
    "        return pixel_acc, dice, precision, recall, IoU\n",
    "\n",
    "\n",
    "class BinaryMetrics():\n",
    "    r\"\"\"Calculate common metrics in binary cases.\n",
    "    In binary cases it should be noted that y_pred shape shall be like (N, 1, H, W), or an assertion \n",
    "    error will be raised.\n",
    "    Also this calculator provides the function to calculate specificity, also known as true negative \n",
    "    rate, as specificity/TPR is meaningless in multiclass cases.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-5, activation='0-1'):\n",
    "        self.eps = eps\n",
    "        self.activation = activation\n",
    "\n",
    "    def _calculate_overlap_metrics(self, gt, pred):\n",
    "        output = pred.view(-1, )\n",
    "        target = gt.view(-1, ).float()\n",
    "\n",
    "        tp = torch.sum(output * target)  # TP\n",
    "        fp = torch.sum(output * (1 - target))  # FP\n",
    "        fn = torch.sum((1 - output) * target)  # FN\n",
    "        tn = torch.sum((1 - output) * (1 - target))  # TN\n",
    "\n",
    "        pixel_acc = (tp + tn + self.eps) / (tp + tn + fp + fn + self.eps)\n",
    "        dice = (2 * tp + self.eps) / (2 * tp + fp + fn + self.eps)\n",
    "        precision = (tp + self.eps) / (tp + fp + self.eps)\n",
    "        recall = (tp + self.eps) / (tp + fn + self.eps)\n",
    "        specificity = (tn + self.eps) / (tn + fp + self.eps)\n",
    "\n",
    "        return pixel_acc, dice, precision, specificity, recall\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # y_true: (N, H, W)\n",
    "        # y_pred: (N, 1, H, W)\n",
    "        if self.activation in [None, 'none']:\n",
    "            activation_fn = lambda x: x\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            activation_fn = nn.Sigmoid()\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"0-1\":\n",
    "            sigmoid_pred = nn.Sigmoid()(y_pred)\n",
    "            activated_pred = (sigmoid_pred > 0.5).float().to(y_pred.device)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not a supported activation!\")\n",
    "\n",
    "        assert activated_pred.shape[1] == 1, 'Predictions must contain only one channel' \\\n",
    "                                             ' when performing binary segmentation'\n",
    "        pixel_acc, dice, precision, specificity, recall = self._calculate_overlap_metrics(y_true.to(y_pred.device,\n",
    "                                                                                                    dtype=torch.float),\n",
    "                                                                                          activated_pred)\n",
    "        return [pixel_acc, dice, precision, specificity, recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d8b09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SegmentationMetrics(object):\n",
    "    r\"\"\"Calculate common metrics in semantic segmentation to evalueate model preformance.\n",
    "    Supported metrics: Pixel accuracy, Dice Coeff, precision score and recall score.\n",
    "    \n",
    "    Pixel accuracy measures how many pixels in a image are predicted correctly.\n",
    "    Dice Coeff is a measure function to measure similarity over 2 sets, which is usually used to\n",
    "    calculate the similarity of two samples. Dice equals to f1 score in semantic segmentation tasks.\n",
    "    \n",
    "    It should be noted that Dice Coeff and Intersection over Union are highly related, so you need \n",
    "    NOT calculate these metrics both, the other can be calcultaed directly when knowing one of them.\n",
    "    Precision describes the purity of our positive detections relative to the ground truth. Of all\n",
    "    the objects that we predicted in a given image, precision score describes how many of those objects\n",
    "    actually had a matching ground truth annotation.\n",
    "    Recall describes the completeness of our positive predictions relative to the ground truth. Of\n",
    "    all the objected annotated in our ground truth, recall score describes how many true positive instances\n",
    "    we have captured in semantic segmentation.\n",
    "    Args:\n",
    "        eps: float, a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        average: bool. Default: ``True``\n",
    "            When set to ``True``, average Dice Coeff, precision and recall are\n",
    "            returned. Otherwise Dice Coeff, precision and recall of each class\n",
    "            will be returned as a numpy array.\n",
    "        ignore_background: bool. Default: ``True``\n",
    "            When set to ``True``, the class will not calculate related metrics on\n",
    "            background pixels. When the segmentation of background pixels is not\n",
    "            important, set this value to ``True``.\n",
    "        activation: [None, 'none', 'softmax' (default), 'sigmoid', '0-1']\n",
    "            This parameter determines what kind of activation function that will be\n",
    "            applied on model output.\n",
    "    Input:\n",
    "        y_true: :math:`(N, H, W)`, torch tensor, where we use int value between (0, num_class - 1)\n",
    "        to denote every class, where ``0`` denotes background class.\n",
    "        y_pred: :math:`(N, C, H, W)`, torch tensor.\n",
    "    Examples::\n",
    "        >>> metric_calculator = SegmentationMetrics(average=True, ignore_background=True)\n",
    "        >>> pixel_accuracy, dice, precision, recall = metric_calculator(y_true, y_pred)\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-5, average=True, ignore_background=True, activation='0-1'):\n",
    "        self.eps = eps\n",
    "        self.average = average\n",
    "        self.ignore = ignore_background\n",
    "        self.activation = activation\n",
    "\n",
    "    @staticmethod\n",
    "    def _one_hot(gt, pred, class_num):\n",
    "        # transform sparse mask into one-hot mask\n",
    "        # shape: (B, H, W) -> (B, C, H, W)\n",
    "        input_shape = tuple(gt.shape)  # (N, H, W, ...)\n",
    "        new_shape = (input_shape[0], class_num) + input_shape[1:]\n",
    "        one_hot = torch.zeros(new_shape).to(pred.device, dtype=torch.float)\n",
    "        target = one_hot.scatter_(1, gt.unsqueeze(1).long().data, 1.0)\n",
    "        return target\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_class_data(gt_onehot, pred, class_num):\n",
    "        # perform calculation on a batch\n",
    "        # for precise result in a single image, plz set batch size to 1\n",
    "        matrix = np.zeros((3, class_num))\n",
    "\n",
    "        # calculate tp, fp, fn per class\n",
    "        for i in range(class_num):\n",
    "            # pred shape: (N, H, W)\n",
    "            class_pred = pred[:, i, :, :]\n",
    "            # gt shape: (N, H, W), binary array where 0 denotes negative and 1 denotes positive\n",
    "            class_gt = gt_onehot[:, i, :, :]\n",
    "\n",
    "            pred_flat = class_pred.contiguous().view(-1, )  # shape: (N * H * W, )\n",
    "            gt_flat = class_gt.contiguous().view(-1, )  # shape: (N * H * W, )\n",
    "\n",
    "            tp = torch.sum(gt_flat * pred_flat)\n",
    "            fp = torch.sum(pred_flat) - tp\n",
    "            fn = torch.sum(gt_flat) - tp\n",
    "\n",
    "            matrix[:, i] = tp.item(), fp.item(), fn.item()\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _calculate_multi_metrics(self, gt, pred, class_num):\n",
    "        # calculate metrics in multi-class segmentation\n",
    "        matrix = self._get_class_data(gt, pred, class_num)\n",
    "        if self.ignore:\n",
    "            matrix = matrix[:, 1:]\n",
    "\n",
    "        # tp = np.sum(matrix[0, :])\n",
    "        # fp = np.sum(matrix[1, :])\n",
    "        # fn = np.sum(matrix[2, :])\n",
    "\n",
    "        pixel_acc = (np.sum(matrix[0, :]) + self.eps) / (np.sum(matrix[0, :]) + np.sum(matrix[1, :]))\n",
    "        dice = (2 * matrix[0] + self.eps) / (2 * matrix[0] + matrix[1] + matrix[2] + self.eps) #F1 Score\n",
    "        precision = (matrix[0] + self.eps) / (matrix[0] + matrix[1] + self.eps)\n",
    "        recall = (matrix[0] + self.eps) / (matrix[0] + matrix[2] + self.eps)\n",
    "        IoU = (matrix[0] + self.eps) / (matrix[0] + matrix[1] + matrix[2] + self.eps)\n",
    "\n",
    "        if self.average:\n",
    "            dice = np.average(dice)\n",
    "            precision = np.average(precision)\n",
    "            recall = np.average(recall)\n",
    "            IoU = np.average(IoU)\n",
    "\n",
    "        return pixel_acc, dice, precision, recall, IoU\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        class_num = y_pred.size(1)\n",
    "\n",
    "        if self.activation in [None, 'none']:\n",
    "            activation_fn = lambda x: x\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            activation_fn = nn.Sigmoid()\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"softmax\":\n",
    "            activation_fn = nn.Softmax(dim=1)\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"0-1\":\n",
    "            pred_argmax = torch.argmax(y_pred, dim=1)\n",
    "            activated_pred = self._one_hot(pred_argmax, y_pred, class_num)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not a supported activation!\")\n",
    "\n",
    "        gt_onehot = self._one_hot(y_true, y_pred, class_num)\n",
    "        pixel_acc, dice, precision, recall, IoU = self._calculate_multi_metrics(gt_onehot, activated_pred, class_num)\n",
    "        return pixel_acc, dice, precision, recall, IoU\n",
    "\n",
    "\n",
    "class BinaryMetrics():\n",
    "    r\"\"\"Calculate common metrics in binary cases.\n",
    "    In binary cases it should be noted that y_pred shape shall be like (N, 1, H, W), or an assertion \n",
    "    error will be raised.\n",
    "    Also this calculator provides the function to calculate specificity, also known as true negative \n",
    "    rate, as specificity/TPR is meaningless in multiclass cases.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-5, activation='0-1'):\n",
    "        self.eps = eps\n",
    "        self.activation = activation\n",
    "\n",
    "    def _calculate_overlap_metrics(self, gt, pred):\n",
    "        output = pred.view(-1, )\n",
    "        target = gt.view(-1, ).float()\n",
    "\n",
    "        tp = torch.sum(output * target)  # TP\n",
    "        fp = torch.sum(output * (1 - target))  # FP\n",
    "        fn = torch.sum((1 - output) * target)  # FN\n",
    "        tn = torch.sum((1 - output) * (1 - target))  # TN\n",
    "\n",
    "        pixel_acc = (tp + tn + self.eps) / (tp + tn + fp + fn + self.eps)\n",
    "        dice = (2 * tp + self.eps) / (2 * tp + fp + fn + self.eps)\n",
    "        precision = (tp + self.eps) / (tp + fp + self.eps)\n",
    "        recall = (tp + self.eps) / (tp + fn + self.eps)\n",
    "        specificity = (tn + self.eps) / (tn + fp + self.eps)\n",
    "\n",
    "        return pixel_acc, dice, precision, specificity, recall\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # y_true: (N, H, W)\n",
    "        # y_pred: (N, 1, H, W)\n",
    "        if self.activation in [None, 'none']:\n",
    "            activation_fn = lambda x: x\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            activation_fn = nn.Sigmoid()\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"0-1\":\n",
    "            sigmoid_pred = nn.Sigmoid()(y_pred)\n",
    "            activated_pred = (sigmoid_pred > 0.5).float().to(y_pred.device)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not a supported activation!\")\n",
    "\n",
    "        assert activated_pred.shape[1] == 1, 'Predictions must contain only one channel' \\\n",
    "                                             ' when performing binary segmentation'\n",
    "        pixel_acc, dice, precision, specificity, recall = self._calculate_overlap_metrics(y_true.to(y_pred.device,\n",
    "                                                                                                    dtype=torch.float),\n",
    "                                                                                          activated_pred)\n",
    "        return [pixel_acc, dice, precision, specificity, recall]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab589b",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93485710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "def loop_fn(mode,dataloader,model,criterion,optimizer,device):\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    elif mode == 'val':\n",
    "        model.eval()\n",
    "    cost = 0\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    F1 = 0\n",
    "    IoU = 0\n",
    "    for sample_batched in tqdm(dataloader, desc=mode.title()):\n",
    "        data, target = sample_batched['data'].to(device), sample_batched['label'].type(torch.LongTensor).to(device) # LongTensor\n",
    "        optimizer.zero_grad()\n",
    "        output,_ = model(data)\n",
    "        output = output.type(torch.Tensor).to(device)\n",
    "        loss = criterion(output, target).to(device)\n",
    "        if mode == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        metric_calculator = SegmentationMetrics(average=True, ignore_background=False, activation='softmax')\n",
    "        cost += criterion(output, target).item()\n",
    "        pixel_accuracy, F1_score, precision, recall,IoU_score = metric_calculator(target, output)\n",
    "        acc += pixel_accuracy.item()\n",
    "        prec += precision.item()\n",
    "        rec += recall.item()\n",
    "        F1 += F1_score.item()\n",
    "        IoU += IoU_score.item()\n",
    "         \n",
    "    cost /= (len(dataloader.dataset)/batch_size)\n",
    "    acc  /= (len(dataloader.dataset)/batch_size)\n",
    "    prec /= (len(dataloader.dataset)/batch_size)\n",
    "    rec  /= (len(dataloader.dataset)/batch_size)\n",
    "    F1   /= (len(dataloader.dataset)/batch_size)\n",
    "    IoU  /= (len(dataloader.dataset)/batch_size)\n",
    "    return cost,acc,F1,prec,rec,IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e0afbb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4a69209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd790f63d3541d3a11c4d2c68899fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/22915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86a090768914f15a5c5877dd927da88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/2865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch     1\n",
      "Train_cost  = 0.0167 | Test_score = 0.9812 |F1_score = 0.7863 |Precision_score = 0.7350 |Recall_score = 0.8705 |IoU_score = 0.6966 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bd20fe04cd424983624d14092cedd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/22915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8995def20354ce194cf38ab8d625a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/2865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch     2\n",
      "Train_cost  = 0.0156 | Test_score = 0.9815 |F1_score = 0.7874 |Precision_score = 0.7369 |Recall_score = 0.8690 |IoU_score = 0.6977 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34462394ac34a05bf222f8d42bf9fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/22915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f725ca61bb44aab0c79e17d97a9677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/2865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch     3\n",
      "Train_cost  = 0.0153 | Test_score = 0.9815 |F1_score = 0.7875 |Precision_score = 0.7371 |Recall_score = 0.8691 |IoU_score = 0.6978 |\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e265ed351843d7926ed96e235c9726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/22915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706f01d28655485fbe53fbf4efcbf2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/2865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch     4\n",
      "Train_cost  = 0.0150 | Test_score = 0.9817 |F1_score = 0.7878 |Precision_score = 0.7387 |Recall_score = 0.8660 |IoU_score = 0.6982 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb03301ddda34bcdb905c377ba95dab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/22915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24205a14806a4006b94c840b739d4e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/2865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch     5\n",
      "Train_cost  = 0.0149 | Test_score = 0.9814 |F1_score = 0.7872 |Precision_score = 0.7364 |Recall_score = 0.8697 |IoU_score = 0.6975 |\n",
      "\u001b[31m==> EarlyStop patience =  1 | Best F1_score: 0.7878\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "\n",
    "for i in range(1,epoch+1):\n",
    "    train_cost,_,_,_,_,_ = loop_fn('train',trainloader,model,criterion,optimizer,device)\n",
    "    with torch.no_grad():\n",
    "        _,test_score,F1_score,Precision_score,Recall_score,IoU_score = loop_fn('val',val_loader,model,criterion,optimizer,device)\n",
    "\n",
    "    #logging\n",
    "    #callback.log(test_score,None,train_cost,)\n",
    "    callback.log(train_cost = train_cost, test_cost=None, train_score=None, test_score=test_score, F1_score=F1_score, Precision_score=Precision_score, Recall_score=Recall_score, IoU_score=IoU_score)\n",
    "    #Checkpoint\n",
    "    callback.save_checkpoint()\n",
    "    #runtime plotting\n",
    "    callback.cost_runtime_plotting()\n",
    "    callback.score_runtime_plotting()\n",
    "    #Early stopping\n",
    "    if callback.early_stopping(model,monitor='F1_score'):\n",
    "        callback.plot_cost()\n",
    "        callback.plot_score()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbec6862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43c40b7796b471891b69285649caf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.985568607284308\n",
      "0.6946415812258022\n",
      "0.7914162743497112\n",
      "0.7324999430899747\n",
      "[0.0420076847076416]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "convert_tensor = T.ToTensor()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    cost = 0\n",
    "    acc = 0\n",
    "    lst_acc = []\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    F1 = 0\n",
    "    IoU = 0\n",
    "    runtime = []\n",
    "    for sample_batched in tqdm(val_loader, 'val'):\n",
    "        data, target = sample_batched['data'].to(device), sample_batched['label'].type(torch.LongTensor).to(device) # LongTensor\n",
    "        #optimizer.zero_grad()\n",
    "        #start = time.time()\n",
    "        output,_ = model(data)\n",
    "        #end = time.time()\n",
    "        #runtime.append((end-start))\n",
    "        metric_calculator = SegmentationMetrics(average=True, ignore_background=False, activation='softmax')\n",
    "        pixel_accuracy, F1_score, precision, recall,IoU_score = metric_calculator(target, output)\n",
    "        lst_acc.append(pixel_accuracy.item())\n",
    "        acc += pixel_accuracy.item()\n",
    "        prec += precision.item()\n",
    "        rec += recall.item()\n",
    "        F1 += F1_score.item()\n",
    "        IoU += IoU_score.item()\n",
    "    \n",
    "    start = time.time()\n",
    "    output,_ = model(data)\n",
    "    end = time.time()\n",
    "    runtime.append((end-start))\n",
    "\n",
    "        \n",
    "#print(acc)\n",
    "#print(prec)\n",
    "#print(rec)\n",
    "#print(IoU)\n",
    "#print(F1)         \n",
    "acc  /= 28\n",
    "prec /= 28\n",
    "rec  /= 28\n",
    "F1   /= 28\n",
    "IoU  /= 28\n",
    "print(acc)\n",
    "print(prec)\n",
    "print(rec)\n",
    "print(F1) \n",
    "print(runtime)\n",
    "#print(IoU)\n",
    "\n",
    "#len(val_loader.dataset)/batch_size\n",
    "#len(lst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36481297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04100942611694336]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f5b0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def readTxt(file_path):\n",
    "    img_list = []\n",
    "    with open(file_path, 'r') as file_to_read:\n",
    "        while True:\n",
    "            lines = file_to_read.readline()\n",
    "            #print(lines)\n",
    "            if not lines:\n",
    "                break\n",
    "            item = lines.strip().split()\n",
    "            img_list.append(item)\n",
    "    file_to_read.close()\n",
    "    return img_list\n",
    "\n",
    "readTxt(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553df39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
