{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3825f77",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b8a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "#import config\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def readTxt(file_path):\n",
    "    img_list = []\n",
    "    with open(file_path, 'r') as file_to_read:\n",
    "        while True:\n",
    "            lines = file_to_read.readline()\n",
    "            if not lines:\n",
    "                break\n",
    "            item = lines.strip().split()\n",
    "            img_list.append(item)\n",
    "    file_to_read.close()\n",
    "    return img_list\n",
    "\n",
    "class RoadSequenceDatasetList(Dataset): #buat unet-convlstm dan segnet-convlstm\n",
    "\n",
    "    def __init__(self, file_path, transforms):\n",
    "\n",
    "        self.img_list = readTxt(file_path)\n",
    "        self.dataset_size = len(self.img_list)\n",
    "        self.transforms = transforms\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path_list = self.img_list[idx]\n",
    "        data = []\n",
    "        for i in range(5):\n",
    "            data.append(torch.unsqueeze(self.transforms(Image.open(img_path_list[i])), dim=0))\n",
    "        data = torch.cat(data, 0)\n",
    "        label = Image.open(img_path_list[5])\n",
    "        label = torch.squeeze(self.transforms(label))\n",
    "        sample = {'data': data, 'label': label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e24fb",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6bd7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_path = 'D:/Kuliah/SEMESTER 8/Robust-Lane-Detection-master/Robust-Lane-Detection-master/LaneDetectionCode/data/train_index.txt'\n",
    "val_path = 'D:/Kuliah/SEMESTER 8/Robust-Lane-Detection-master/Robust-Lane-Detection-master/LaneDetectionCode/data/val_index.txt'\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "batch_size = 1;\n",
    "from torch.utils.data import DataLoader\n",
    "train_set = RoadSequenceDatasetList(file_path=train_path,transforms=transform)\n",
    "trainloader = DataLoader(train_set,batch_size=batch_size,shuffle=True)\n",
    "val_set = RoadSequenceDatasetList(file_path=val_path,transforms=transform)\n",
    "val_loader = DataLoader(val_set,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89fd9c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b2ade",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b7043",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42df7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffX = x1.size()[2] - x2.size()[2]\n",
    "        diffY = x1.size()[3] - x2.size()[3]\n",
    "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
    "                        diffY // 2, int(diffY / 2)))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: (int, int)\n",
    "            Height and width of input tensor as (height, width).\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.height, self.width = input_size\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, self.height, self.width).cuda(),\n",
    "                torch.zeros(batch_size, self.hidden_dim, self.height, self.width).cuda())\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.height, self.width = input_size\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_size=(self.height, self.width),\n",
    "                                          input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor=input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "\n",
    "\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output = layer_output.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7b829",
   "metadata": {},
   "source": [
    "# SegNet-ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c9b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SegNet_ConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegNet_ConvLSTM,self).__init__()\n",
    "        self.vgg16_bn = models.vgg16_bn(pretrained=True).features\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.index_MaxPool = nn.MaxPool2d(kernel_size=2, stride=2,return_indices=True)\n",
    "        self.index_UnPool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        # net struct\n",
    "        self.conv1_block = nn.Sequential(self.vgg16_bn[0],  # conv2d(3,64,(3,3))\n",
    "                                         self.vgg16_bn[1],  # bn(64,eps=1e-05,momentum=0.1,affine=True)\n",
    "                                         self.vgg16_bn[2],  # relu(in_place)\n",
    "                                         self.vgg16_bn[3],  # conv2d(3,64,(3,3))\n",
    "                                         self.vgg16_bn[4],  # bn(64,eps=1e-05,momentum=0.1,affine=True)\n",
    "                                         self.vgg16_bn[5]   # relu(in_place)\n",
    "                                         )\n",
    "        self.conv2_block = nn.Sequential(self.vgg16_bn[7],\n",
    "                                         self.vgg16_bn[8],\n",
    "                                         self.vgg16_bn[9],\n",
    "                                         self.vgg16_bn[10],\n",
    "                                         self.vgg16_bn[11],\n",
    "                                         self.vgg16_bn[12]\n",
    "                                         )\n",
    "        self.conv3_block = nn.Sequential(self.vgg16_bn[14],\n",
    "                                         self.vgg16_bn[15],\n",
    "                                         self.vgg16_bn[16],\n",
    "                                         self.vgg16_bn[17],\n",
    "                                         self.vgg16_bn[18],\n",
    "                                         self.vgg16_bn[19],\n",
    "                                         self.vgg16_bn[20],\n",
    "                                         self.vgg16_bn[21],\n",
    "                                         self.vgg16_bn[22]\n",
    "                                         )\n",
    "        self.conv4_block = nn.Sequential(self.vgg16_bn[24],\n",
    "                                         self.vgg16_bn[25],\n",
    "                                         self.vgg16_bn[26],\n",
    "                                         self.vgg16_bn[27],\n",
    "                                         self.vgg16_bn[28],\n",
    "                                         self.vgg16_bn[29],\n",
    "                                         self.vgg16_bn[30],\n",
    "                                         self.vgg16_bn[31],\n",
    "                                         self.vgg16_bn[32]\n",
    "                                         )\n",
    "        self.conv5_block = nn.Sequential(self.vgg16_bn[34],\n",
    "                                         self.vgg16_bn[35],\n",
    "                                         self.vgg16_bn[36],\n",
    "                                         self.vgg16_bn[37],\n",
    "                                         self.vgg16_bn[38],\n",
    "                                         self.vgg16_bn[39],\n",
    "                                         self.vgg16_bn[40],\n",
    "                                         self.vgg16_bn[41],\n",
    "                                         self.vgg16_bn[42]\n",
    "                                         )\n",
    "\n",
    "        self.upconv5_block = nn.Sequential(\n",
    "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1,1)),\n",
    "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                           self.relu,\n",
    "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1,1)),\n",
    "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                           self.relu,\n",
    "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1, 1)),\n",
    "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                           self.relu,\n",
    "                                           )\n",
    "        self.upconv4_block = nn.Sequential(\n",
    "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1,1)),\n",
    "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                           self.relu,\n",
    "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1,1)),\n",
    "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                           self.relu,\n",
    "                                           nn.Conv2d(512, 256, kernel_size=(3, 3), padding=(1, 1)),\n",
    "                                           nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                           self.relu,\n",
    "                                           )\n",
    "        self.upconv3_block = nn.Sequential(\n",
    "                                           nn.Conv2d(256, 256, kernel_size=(3, 3), padding=(1,1)),\n",
    "                                           nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                           self.relu,\n",
    "                                           nn.Conv2d(256, 256, kernel_size=(3, 3), padding=(1,1)),\n",
    "                                           nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                           self.relu,\n",
    "                                           nn.Conv2d(256, 128, kernel_size=(3, 3), padding=(1, 1)),\n",
    "                                           nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                           self.relu,\n",
    "                                           )\n",
    "        self.upconv2_block = nn.Sequential(\n",
    "                                           nn.Conv2d(128, 128, kernel_size=(3, 3), padding=(1,1)),\n",
    "                                           nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                           self.relu,\n",
    "                                           nn.Conv2d(128, 64, kernel_size=(3, 3), padding=(1,1)),\n",
    "                                           nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                           self.relu\n",
    "                                           )\n",
    "        self.upconv1_block = nn.Sequential(\n",
    "                                           nn.Conv2d(64, 64, kernel_size=(3, 3), padding=(1,1)),\n",
    "                                           nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                           self.relu,\n",
    "                                           nn.Conv2d(64, 2, kernel_size=(3, 3), padding=(1,1)),\n",
    "                                           )\n",
    "        self.convlstm = ConvLSTM(input_size=(4,8),\n",
    "                                 input_dim=512,\n",
    "                                 hidden_dim=[512, 512],\n",
    "                                 kernel_size=(3,3),\n",
    "                                 num_layers=2,\n",
    "                                 batch_first=False,\n",
    "                                 bias=True,\n",
    "                                 return_all_layers=False)\n",
    "    def forward(self, x):\n",
    "        x = torch.unbind(x, dim=1)\n",
    "        data = []\n",
    "        for item in x:\n",
    "            f1, idx1 = self.index_MaxPool(self.conv1_block(item))\n",
    "            f2, idx2 = self.index_MaxPool(self.conv2_block(f1))\n",
    "            f3, idx3 = self.index_MaxPool(self.conv3_block(f2))\n",
    "            f4, idx4 = self.index_MaxPool(self.conv4_block(f3))\n",
    "            f5, idx5 = self.index_MaxPool(self.conv5_block(f4))\n",
    "            data.append(f5.unsqueeze(0))\n",
    "        data = torch.cat(data, dim=0)\n",
    "        lstm, _ = self.convlstm(data)\n",
    "        test = lstm[0][-1,:,:,:,:]\n",
    "        up6 = self.index_UnPool(test,idx5)\n",
    "        up5 = self.index_UnPool(self.upconv5_block(up6), idx4)\n",
    "        up4 = self.index_UnPool(self.upconv4_block(up5), idx3)\n",
    "        up3 = self.index_UnPool(self.upconv3_block(up4), idx2)\n",
    "        up2 = self.index_UnPool(self.upconv2_block(up3), idx1)\n",
    "        up1 = self.upconv1_block(up2)\n",
    "        return F.log_softmax(up1, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef7083",
   "metadata": {},
   "source": [
    "# Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed73f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch\n",
    "#from jcopdl.callback import Callback, set_config\n",
    "from _callback import CallbackSemSeg\n",
    "from _config import set_config\n",
    "learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6452fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = set_config({\n",
    "    'batch_size':batch_size,\n",
    "    'learning_rate':learning_rate\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7fd7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Kuliah/DeepLearning/Code/SemanticSegmentation/4_SemanticSegmentationTry/Tugas Akhir/2_TA_SegNetConvLSTM/Epoch74/weights_best.pth'\n",
    "model = SegNet_ConvLSTM().to(device)\n",
    "model.load_state_dict(torch.load(path))\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor([0.02, 2.02]).to(device))\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "callback = CallbackSemSeg(model,config,outdir='Tugas Akhir/2_TA_SegNetConvLSTM/Epoch80',plot_every=3,save_every=1,early_stop_patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46c232",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "799648c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SegmentationMetrics(object):\n",
    "    r\"\"\"Calculate common metrics in semantic segmentation to evalueate model preformance.\n",
    "    Supported metrics: Pixel accuracy, Dice Coeff, precision score and recall score.\n",
    "    \n",
    "    Pixel accuracy measures how many pixels in a image are predicted correctly.\n",
    "    Dice Coeff is a measure function to measure similarity over 2 sets, which is usually used to\n",
    "    calculate the similarity of two samples. Dice equals to f1 score in semantic segmentation tasks.\n",
    "    \n",
    "    It should be noted that Dice Coeff and Intersection over Union are highly related, so you need \n",
    "    NOT calculate these metrics both, the other can be calcultaed directly when knowing one of them.\n",
    "    Precision describes the purity of our positive detections relative to the ground truth. Of all\n",
    "    the objects that we predicted in a given image, precision score describes how many of those objects\n",
    "    actually had a matching ground truth annotation.\n",
    "    Recall describes the completeness of our positive predictions relative to the ground truth. Of\n",
    "    all the objected annotated in our ground truth, recall score describes how many true positive instances\n",
    "    we have captured in semantic segmentation.\n",
    "    Args:\n",
    "        eps: float, a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        average: bool. Default: ``True``\n",
    "            When set to ``True``, average Dice Coeff, precision and recall are\n",
    "            returned. Otherwise Dice Coeff, precision and recall of each class\n",
    "            will be returned as a numpy array.\n",
    "        ignore_background: bool. Default: ``True``\n",
    "            When set to ``True``, the class will not calculate related metrics on\n",
    "            background pixels. When the segmentation of background pixels is not\n",
    "            important, set this value to ``True``.\n",
    "        activation: [None, 'none', 'softmax' (default), 'sigmoid', '0-1']\n",
    "            This parameter determines what kind of activation function that will be\n",
    "            applied on model output.\n",
    "    Input:\n",
    "        y_true: :math:`(N, H, W)`, torch tensor, where we use int value between (0, num_class - 1)\n",
    "        to denote every class, where ``0`` denotes background class.\n",
    "        y_pred: :math:`(N, C, H, W)`, torch tensor.\n",
    "    Examples::\n",
    "        >>> metric_calculator = SegmentationMetrics(average=True, ignore_background=True)\n",
    "        >>> pixel_accuracy, dice, precision, recall = metric_calculator(y_true, y_pred)\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-5, average=True, ignore_background=True, activation='0-1'):\n",
    "        self.eps = eps\n",
    "        self.average = average\n",
    "        self.ignore = ignore_background\n",
    "        self.activation = activation\n",
    "\n",
    "    @staticmethod\n",
    "    def _one_hot(gt, pred, class_num):\n",
    "        # transform sparse mask into one-hot mask\n",
    "        # shape: (B, H, W) -> (B, C, H, W)\n",
    "        input_shape = tuple(gt.shape)  # (N, H, W, ...)\n",
    "        new_shape = (input_shape[0], class_num) + input_shape[1:]\n",
    "        one_hot = torch.zeros(new_shape).to(pred.device, dtype=torch.float)\n",
    "        target = one_hot.scatter_(1, gt.unsqueeze(1).long().data, 1.0)\n",
    "        return target\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_class_data(gt_onehot, pred, class_num):\n",
    "        # perform calculation on a batch\n",
    "        # for precise result in a single image, plz set batch size to 1\n",
    "        matrix = np.zeros((3, class_num))\n",
    "\n",
    "        # calculate tp, fp, fn per class\n",
    "        for i in range(class_num):\n",
    "            # pred shape: (N, H, W)\n",
    "            class_pred = pred[:, i, :, :]\n",
    "            # gt shape: (N, H, W), binary array where 0 denotes negative and 1 denotes positive\n",
    "            class_gt = gt_onehot[:, i, :, :]\n",
    "\n",
    "            pred_flat = class_pred.contiguous().view(-1, )  # shape: (N * H * W, )\n",
    "            gt_flat = class_gt.contiguous().view(-1, )  # shape: (N * H * W, )\n",
    "\n",
    "            tp = torch.sum(gt_flat * pred_flat)\n",
    "            fp = torch.sum(pred_flat) - tp\n",
    "            fn = torch.sum(gt_flat) - tp\n",
    "\n",
    "            matrix[:, i] = tp.item(), fp.item(), fn.item()\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _calculate_multi_metrics(self, gt, pred, class_num):\n",
    "        # calculate metrics in multi-class segmentation\n",
    "        matrix = self._get_class_data(gt, pred, class_num)\n",
    "        if self.ignore:\n",
    "            matrix = matrix[:, 1:]\n",
    "\n",
    "        # tp = np.sum(matrix[0, :])\n",
    "        # fp = np.sum(matrix[1, :])\n",
    "        # fn = np.sum(matrix[2, :])\n",
    "\n",
    "        pixel_acc = (np.sum(matrix[0, :]) + self.eps) / (np.sum(matrix[0, :]) + np.sum(matrix[1, :]))\n",
    "        dice = (2 * matrix[0] + self.eps) / (2 * matrix[0] + matrix[1] + matrix[2] + self.eps) #F1 Score\n",
    "        precision = (matrix[0] + self.eps) / (matrix[0] + matrix[1] + self.eps)\n",
    "        recall = (matrix[0] + self.eps) / (matrix[0] + matrix[2] + self.eps)\n",
    "        IoU = (matrix[0] + self.eps) / (matrix[0] + matrix[1] + matrix[2] + self.eps)\n",
    "\n",
    "        if self.average:\n",
    "            dice = np.average(dice)\n",
    "            precision = np.average(precision)\n",
    "            recall = np.average(recall)\n",
    "            IoU = np.average(IoU)\n",
    "\n",
    "        return pixel_acc, dice, precision, recall, IoU\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        class_num = y_pred.size(1)\n",
    "\n",
    "        if self.activation in [None, 'none']:\n",
    "            activation_fn = lambda x: x\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            activation_fn = nn.Sigmoid()\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"softmax\":\n",
    "            activation_fn = nn.Softmax(dim=1)\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"0-1\":\n",
    "            pred_argmax = torch.argmax(y_pred, dim=1)\n",
    "            activated_pred = self._one_hot(pred_argmax, y_pred, class_num)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not a supported activation!\")\n",
    "\n",
    "        gt_onehot = self._one_hot(y_true, y_pred, class_num)\n",
    "        pixel_acc, dice, precision, recall, IoU = self._calculate_multi_metrics(gt_onehot, activated_pred, class_num)\n",
    "        return pixel_acc, dice, precision, recall, IoU\n",
    "\n",
    "\n",
    "class BinaryMetrics():\n",
    "    r\"\"\"Calculate common metrics in binary cases.\n",
    "    In binary cases it should be noted that y_pred shape shall be like (N, 1, H, W), or an assertion \n",
    "    error will be raised.\n",
    "    Also this calculator provides the function to calculate specificity, also known as true negative \n",
    "    rate, as specificity/TPR is meaningless in multiclass cases.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-5, activation='0-1'):\n",
    "        self.eps = eps\n",
    "        self.activation = activation\n",
    "\n",
    "    def _calculate_overlap_metrics(self, gt, pred):\n",
    "        output = pred.view(-1, )\n",
    "        target = gt.view(-1, ).float()\n",
    "\n",
    "        tp = torch.sum(output * target)  # TP\n",
    "        fp = torch.sum(output * (1 - target))  # FP\n",
    "        fn = torch.sum((1 - output) * target)  # FN\n",
    "        tn = torch.sum((1 - output) * (1 - target))  # TN\n",
    "\n",
    "        pixel_acc = (tp + tn + self.eps) / (tp + tn + fp + fn + self.eps)\n",
    "        dice = (2 * tp + self.eps) / (2 * tp + fp + fn + self.eps)\n",
    "        precision = (tp + self.eps) / (tp + fp + self.eps)\n",
    "        recall = (tp + self.eps) / (tp + fn + self.eps)\n",
    "        specificity = (tn + self.eps) / (tn + fp + self.eps)\n",
    "\n",
    "        return pixel_acc, dice, precision, specificity, recall\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # y_true: (N, H, W)\n",
    "        # y_pred: (N, 1, H, W)\n",
    "        if self.activation in [None, 'none']:\n",
    "            activation_fn = lambda x: x\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            activation_fn = nn.Sigmoid()\n",
    "            activated_pred = activation_fn(y_pred)\n",
    "        elif self.activation == \"0-1\":\n",
    "            sigmoid_pred = nn.Sigmoid()(y_pred)\n",
    "            activated_pred = (sigmoid_pred > 0.5).float().to(y_pred.device)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not a supported activation!\")\n",
    "\n",
    "        assert activated_pred.shape[1] == 1, 'Predictions must contain only one channel' \\\n",
    "                                             ' when performing binary segmentation'\n",
    "        pixel_acc, dice, precision, specificity, recall = self._calculate_overlap_metrics(y_true.to(y_pred.device,\n",
    "                                                                                                    dtype=torch.float),\n",
    "                                                                                          activated_pred)\n",
    "        return [pixel_acc, dice, precision, specificity, recall]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9dfc6",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34472386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "def loop_fn(mode,dataloader,model,criterion,optimizer,device):\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    elif mode == 'val':\n",
    "        model.eval()\n",
    "    cost = 0\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    F1 = 0\n",
    "    IoU = 0\n",
    "    for sample_batched in tqdm(dataloader, desc=mode.title()):\n",
    "        data, target = sample_batched['data'].to(device), sample_batched['label'].type(torch.LongTensor).to(device) # LongTensor\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = output.type(torch.Tensor).to(device)\n",
    "        loss = criterion(output, target).to(device)\n",
    "        if mode == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        metric_calculator = SegmentationMetrics(average=True, ignore_background=False, activation='softmax')\n",
    "        cost += criterion(output, target).item()\n",
    "        pixel_accuracy, F1_score, precision, recall,IoU_score = metric_calculator(target, output)\n",
    "        acc += pixel_accuracy.item()\n",
    "        prec += precision.item()\n",
    "        rec += recall.item()\n",
    "        F1 += F1_score.item()\n",
    "        IoU += IoU_score.item()\n",
    "         \n",
    "    cost /= (len(dataloader.dataset)/batch_size)\n",
    "    acc  /= (len(dataloader.dataset)/batch_size)\n",
    "    prec /= (len(dataloader.dataset)/batch_size)\n",
    "    rec  /= (len(dataloader.dataset)/batch_size)\n",
    "    F1   /= (len(dataloader.dataset)/batch_size)\n",
    "    IoU  /= (len(dataloader.dataset)/batch_size)\n",
    "    return cost,acc,F1,prec,rec,IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f44189",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7ed8406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5975b91654e44a7a55222105bbefd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/22915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MCENDE~1\\AppData\\Local\\Temp/ipykernel_21380/2330187041.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain_cost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mF1_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPrecision_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRecall_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIoU_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\MCENDE~1\\AppData\\Local\\Temp/ipykernel_21380/2170718447.py\u001b[0m in \u001b[0;36mloop_fn\u001b[1;34m(mode, dataloader, model, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# LongTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\PyTorch1.10.2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\MCENDE~1\\AppData\\Local\\Temp/ipykernel_21380/3081635350.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mlstm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mup6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_UnPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0midx5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\PyTorch1.10.2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\MCENDE~1\\AppData\\Local\\Temp/ipykernel_21380/3094182110.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_tensor, hidden_state)\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mlayer_output_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\MCENDE~1\\AppData\\Local\\Temp/ipykernel_21380/3094182110.py\u001b[0m in \u001b[0;36m_init_hidden\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[0minit_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0minit_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\MCENDE~1\\AppData\\Local\\Temp/ipykernel_21380/3094182110.py\u001b[0m in \u001b[0;36minit_hidden\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         return (torch.zeros(batch_size, self.hidden_dim, self.height, self.width).cuda(),\n\u001b[0m\u001b[0;32m    139\u001b[0m                 torch.zeros(batch_size, self.hidden_dim, self.height, self.width).cuda())\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 15\n",
    "\n",
    "for i in range(1,epoch+1):\n",
    "    train_cost,_,_,_,_,_ = loop_fn('train',trainloader,model,criterion,optimizer,device)\n",
    "    with torch.no_grad():\n",
    "        _,test_score,F1_score,Precision_score,Recall_score,IoU_score = loop_fn('val',val_loader,model,criterion,optimizer,device)\n",
    "\n",
    "    #logging\n",
    "    #callback.log(test_score,None,train_cost,)\n",
    "    callback.log(train_cost = train_cost, test_cost=None, train_score=None, test_score=test_score, F1_score=F1_score, Precision_score=Precision_score, Recall_score=Recall_score, IoU_score=IoU_score)\n",
    "    #Checkpoint\n",
    "    callback.save_checkpoint()\n",
    "    #runtime plotting\n",
    "    callback.cost_runtime_plotting()\n",
    "    callback.score_runtime_plotting()\n",
    "    #Early stopping\n",
    "    if callback.early_stopping(model,monitor='F1_score'):\n",
    "        callback.plot_cost()\n",
    "        callback.plot_score()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0f2b05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e6db9e8459447caf1abbec54bba689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/11457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        _,test_score,F1_score,Precision_score,Recall_score,IoU_score = loop_fn('val',val_loader,model,criterion,optimizer,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4ee88cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7234229333925888"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde44a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
